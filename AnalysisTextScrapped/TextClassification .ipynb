{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying content of job posting to recognize if section mentioned is regarding job requirements  \n",
    "\n",
    "#### A List of 459 words were scraped as key words from the primary scrapper. These words were then cleaned of characters, cleaned to remove stop words, tokenized and the top 100 commonly occuring words were gathered. <br> The words of interest where then checked against the top 100 common words and if yes, they are added to the features  and target dataframe and are one-hot-encoded. A simple logistic regression model was run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for cleaning up data and doing a simple count on the most commonly occuring values \n",
    "def clean_file_text(text):\n",
    "    new_text = re.sub('\\n', '', text)\n",
    "    new_text = re.sub('%', '', new_text)\n",
    "    new_text = re.sub('@', '', new_text)\n",
    "    new_text = re.sub(r'[0-9]', '', new_text)\n",
    "    new_text = new_text.lower()\n",
    "    return new_text\n",
    "def remove_stop_words(text):\n",
    "    clean_text = []\n",
    "    for t in text: \n",
    "        if t in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            clean_text.append(t)\n",
    "    return(clean_text)\n",
    "def corpus_count_words(uploadedFile):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Chunking each word\n",
    "    word_counter = Counter()\n",
    "    for file in uploadedFile:\n",
    "        file_data = clean_file_text(file)\n",
    "        file_words = tokenizer.tokenize(file_data)\n",
    "        non_stopwords = remove_stop_words(file_words)\n",
    "        word_counter.update(non_stopwords)\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of commonly occuring stop words from http://www.nltk.org/nltk_data/\n",
    "stopwords = open(\"english\", \"r\")\n",
    "stopwords = [clean_file_text(word) for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'common100words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fc711499f070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon100words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'common100words' is not defined"
     ]
    }
   ],
   "source": [
    "common100words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qualification = pd.read_csv('qualification.csv', sep=',',header=0)\n",
    "commonlyOccuring = corpus_count_words(qualification['qualifications'])\n",
    "common100words = [words for words in commonlyOccuring.keys()]\n",
    "common100words = common100words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: (460, 100) And length of target: 460\n"
     ]
    }
   ],
   "source": [
    "# Transforming the string data into \n",
    "df_rows = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for row in qualification.iloc[:,0]:\n",
    "    words = tokenizer.tokenize(row)\n",
    "    df_rows.append([1 if word in words else 0 for word in common100words])      \n",
    "    \n",
    "# Transformed dataset and target into binary values \n",
    "X = pd.DataFrame(df_rows, columns = common100words)\n",
    "target = qualification.iloc[:,1]\n",
    "y = [1 if t == 'yes' else 0 for t in target]\n",
    "print(f'Shape of features: {X.shape} And length of target: {len(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X.assign(target_group=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver = 'liblinear', penalty = 'l2').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train and test set\n",
    "y_train_predict = clf.predict(X_train)\n",
    "y_test_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate train and test accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "test_accuracy = accuracy_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 83.54%\n",
      "Test accuracy: 78.99%\n"
     ]
    }
   ],
   "source": [
    "# report results\n",
    "print(f\"Train accuracy: {(train_accuracy*100):.2f}%\")\n",
    "print(f\"Test accuracy: {(test_accuracy*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reg_model.sav']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = 'Reg_model.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(text):\n",
    "    top100words = ['job',\n",
    " 'summary',\n",
    " 'qualifications',\n",
    " 'education',\n",
    " 'training',\n",
    " 'experience',\n",
    " 'health',\n",
    " 'background',\n",
    " 'requirements',\n",
    " 'covid',\n",
    " 'call',\n",
    " 'center',\n",
    " 'representative',\n",
    " 'pay',\n",
    " 'rate',\n",
    " 'hr',\n",
    " 'remote',\n",
    " 'eastern',\n",
    " 'time',\n",
    " 'zone',\n",
    " 'responsibilities',\n",
    " 'position',\n",
    " 'objective',\n",
    " 'special',\n",
    " 'notes',\n",
    " 'essential',\n",
    " 'skills',\n",
    " 'competencies',\n",
    " 'physical',\n",
    " 'company',\n",
    " 'overview',\n",
    " 'mission',\n",
    " 'statement',\n",
    " 'core',\n",
    " 'duties',\n",
    " 'required',\n",
    " 'additional',\n",
    " 'description',\n",
    " 'detail',\n",
    " 'calibration',\n",
    " 'drifts',\n",
    " 'precision',\n",
    " 'calibrations',\n",
    " 'method',\n",
    " 'monitoring',\n",
    " 'documentation',\n",
    " 'reporting',\n",
    " 'daily',\n",
    " 'safety',\n",
    " 'must',\n",
    " 'able',\n",
    " 'travel',\n",
    " 'emsi',\n",
    " 'united',\n",
    " 'states',\n",
    " 'postal',\n",
    " 'service',\n",
    " 'external',\n",
    " 'publication',\n",
    " 'posting',\n",
    " 'branch',\n",
    " 'period',\n",
    " 'title',\n",
    " 'facility',\n",
    " 'location',\n",
    " 'information',\n",
    " 'persons',\n",
    " 'eligible',\n",
    " 'apply',\n",
    " 'check',\n",
    " 'functional',\n",
    " 'purpose',\n",
    " 'business',\n",
    " 'analyst',\n",
    " 'function',\n",
    " 'include',\n",
    " 'limited',\n",
    " 'following',\n",
    " 'minimum',\n",
    " 'abilities',\n",
    " 'disclaimer',\n",
    " 'functions',\n",
    " 'licensure',\n",
    " 'address',\n",
    " 'city',\n",
    " 'state',\n",
    " 'zip',\n",
    " 'code',\n",
    " 'domicile',\n",
    " 'eeo',\n",
    " 'learning',\n",
    " 'management',\n",
    " 'system',\n",
    " 'specialist',\n",
    " 'general',\n",
    " 'key',\n",
    " 'knowledge',\n",
    " 'work',\n",
    " 'environment',\n",
    " 'demands']\n",
    "    clean_text = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word = tokenizer.tokenize(text)\n",
    "    for w in word:\n",
    "        c_word = re.sub('\\n', '', w)\n",
    "        c_word = re.sub('%', '', c_word)\n",
    "        c_word = re.sub('@', '', c_word)\n",
    "        c_word = re.sub(r'[0-9]', '', c_word)\n",
    "        if w not in stopwords:\n",
    "            clean_text.append(w)\n",
    "        else:\n",
    "            pass\n",
    "    df_rows1 = [[1 if word in words else 0 for word in common100words]]\n",
    "    X = pd.DataFrame(df_rows1, columns = common100words)\n",
    "    return (clean_text)\n",
    "    #y_test_predict1 = clf.predict(X)\n",
    "    #test_accuracy = accuracy_score(y_test, y_test_predict1)\n",
    "    #print(f\"Test accuracy: {(test_accuracy*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['job', 'summari'], ['qualif'], ['educ'], ['train', 'experi'], ['health', 'background', 'requir'], ['covid', '19', 'call', 'center', 'repres'], ['pay', 'rate', '13', 'hr'], ['remot', 'eastern', 'time', 'zone'], ['respons'], ['requir'], ['posit', 'object'], ['respons'], ['qualif'], ['special', 'note'], ['summari'], ['essenti', 'respons'], ['qualif'], ['skill', 'compet'], ['physic', 'requir'], ['compani', 'overview'], ['mission', 'statement'], ['job', 'summari'], ['core', 'job', 'duti'], ['requir', 'experi', 'educ', 'and', 'skill'], ['addit', 'requir'], ['respons'], ['descript', 'detail'], ['calibr', 'drift', 'precis', 'calibr', 'method', '21', 'monitor'], ['document', 'report'], ['daili', 'document'], ['physic', 'requir'], ['safeti'], ['must', 'Be', 'abl', 'travel'], ['about', 'emsi'], ['unit', 'state', 'postal', 'servic'], ['extern', 'public', 'job', 'post', '10503639'], ['branch'], ['job', 'post', 'period'], ['job', 'titl'], ['facil', 'locat'], ['posit', 'inform'], ['person', 'elig', 'appli'], ['background', 'check'], ['function', 'purpos'], ['requir'], ['job', 'titl'], ['busi', 'analyst', 'posit', 'function'], ['busi', 'analyst', 'respons', 'To', 'includ', 'limit', 'follow'], ['busi', 'analyst', 'minimum', 'requir'], ['busi', 'analyst', 'abil', 'requir'], ['disclaim'], ['overview'], ['qualif'], ['essenti', 'function'], ['minimum', 'educ'], ['minimum', 'experi'], ['requir', 'skill', 'abil', 'licensur'], ['other', 'job', 'descript', 'inform'], ['travel', 'posit'], ['address'], ['citi'], ['state'], ['zip', 'code'], ['domicil', 'locat'], ['eeo', 'statement'], ['essenti', 'function'], ['minimum', 'educ'], ['minimum', 'experi'], ['requir', 'skill', 'abil', 'licensur'], ['other', 'job', 'descript', 'inform'], ['travel', 'posit'], ['address'], ['citi'], ['state'], ['zip', 'code'], ['domicil', 'locat'], ['eeo', 'statement'], ['learn', 'manag', 'system', 'specialist'], ['gener', 'summari'], ['key', 'respons', 'essenti', 'function'], ['knowledg', 'skill', 'requir'], ['qualif', 'educ', 'requir', 'experi'], ['work', 'environ'], ['physic', 'demand'], ['posit', 'type', 'expect', 'hour', 'work'], ['travel'], ['other', 'duti', 'disclaim'], ['qualif'], ['qualif'], ['minimum', 'job', 'requir'], [], ['prefer', 'qualif'], ['posit', 'criteria', 'what', 'look'], ['job', 'engin'], ['region', 'north', 'america', 'US', 'puerto', 'rico'], ['organ', 'electr', 'sector'], ['job', 'level', 'director'], ['schedul', 'full', 'time'], ['Is', 'remot', 'work', 'e', 'work', 'home', 'anoth', 'eaton', 'facil', 'allow', 'posit', 'No'], ['doe', 'posit', 'offer', 'reloc', 'reloc', 'within', 'unit', 'state', 'puerto', 'rico'], ['travel', 'ye', '10', 'time'], ['field', 'engin', 'scientist'], ['cleanair', 'equal', 'opportun', 'affirm', 'action', 'employ', 'offer', 'challeng', 'career', 'opportun', 'train', 'competit', 'compens', 'drug', 'free', 'smoke', 'free', 'work', 'environ'], ['essenti', 'function'], ['minimum', 'educ'], ['minimum', 'experi'], ['requir', 'skill', 'abil', 'licensur'], ['other', 'job', 'descript', 'inform'], ['travel', 'posit'], ['address'], ['citi'], ['state'], ['zip', 'code'], ['domicil', 'locat'], ['eeo', 'statement'], ['autom', 'engin'], ['respons'], ['requir'], ['bsi', 'core', 'valu'], ['ethic'], ['honest'], ['respect'], ['servic'], ['posit', 'attitud'], ['account'], ['continu', 'learn'], ['take', 'initi'], ['cooper'], ['stewardship'], ['safeti'], ['resourc'], ['about', 'bsi'], ['locat'], ['compani'], ['job', 'titl'], ['job', 'requisit', 'number'], ['categori'], ['locat'], ['sponsorship', 'avail', 'posit'], ['your', 'applic', 'post', 'may', 'consid', 'posit', 'list'], ['full', 'stack', 'develop', 'posit'], ['full', 'stack', 'develop', 'I'], ['full', 'stack', 'develop', 'II'], ['job', 'descript', 'summari'], ['job', 'specif', 'inform'], ['skill', 'knowledg', 'consid', 'plu'], ['minimum', 'qualif'], ['domicil'], ['reloc'], ['applic', 'criteria'], ['who', 'WE', 'are'], ['transport', 'practic'], ['descript'], ['profession', 'requir'], ['qualifi', 'candid', 'encourag', 'email', 'resum'], ['jbelowich', 'mbakerintl', 'com'], ['michael', 'baker', 'intern', 'eeo', 'statement'], ['eaton', 'cpd', 'power', 'compon', 'divis', 'current', 'seek', 'test', 'lab', 'engin', 'join', 'team', 'thi', 'posit', 'base', 'beaver', 'PA', 'locat'], ['posit', 'overview'], ['In', 'function'], ['qualif'], ['requir', 'basic', 'qualif'], ['prefer', 'qualif'], ['posit', 'criteria'], ['job', 'engin'], ['region', 'north', 'america', 'US', 'puerto', 'rico'], ['organ', 'epg', 'cpd', 'pcd', 'power', 'compon', 'divis'], ['job', 'level', 'individu', 'contributor'], ['schedul', 'full', 'time'], ['Is', 'remot', 'work', 'e', 'work', 'home', 'anoth', 'eaton', 'facil', 'allow', 'posit', 'No'], ['doe', 'posit', 'offer', 'reloc', 'No'], ['travel', 'ye', '10', 'time'], ['who', 'WE', 'are'], ['construct', 'practic'], ['descript'], ['profession', 'requir'], ['michael', 'baker', 'intern', 'eeo', 'statement'], ['compani'], ['job', 'titl'], ['job', 'requisit', 'number'], ['categori'], ['locat'], ['deep', 'understand', 'strong', 'experi', 'one', 'follow'], ['conceptu', 'understand', 'experi', 'one', 'follow'], ['domicil'], ['reloc'], ['minimum', 'qualif'], ['applic', 'deadlin', 'criteria'], ['novemb', '20', '2020'], ['job', 'detail'], ['evalu', 'RE', 'requir', 'build', 'RE', 'team', 'scale'], ['overse', 'day', 'day', 'reliabl', 'engin', 'activ', 'across', 'brand', 'channel'], ['staff', 'manag', 'financi', 'plan'], ['qualif'], ['minimum', 'qualif'], ['prefer', 'qualif'], ['job', 'detail'], ['job', 'overview'], ['about', 'learndatasci'], ['respons'], ['requir'], ['titl'], ['posit', 'summari'], ['term'], ['minimum', 'qualif'], ['here', 'vce', 'know', 'biolog', 'commun', 'organ', 'strongest', 'resili', 'divers', 'We', 'invit', 'join', 'us', 'engag', 'us', 'challeng', 'us', 'grow', 'us', 'If', 'thrive', 'support', 'environ', 'encourag', 'follow', 'big', 'idea', 'help', 'other', 'realiz', 'vce', 'place'], ['applic'], ['essenti', 'function'], ['minimum', 'educ'], ['minimum', 'experi'], ['requir', 'skill', 'abil', 'licensur'], ['other', 'job', 'descript', 'inform'], ['address'], ['citi'], ['state'], ['zip', 'code'], ['domicil', 'locat'], ['eeo', 'statement'], ['note', 'lucd', 'sponsor', 'job', 'applic', 'work', 'visa'], ['open', 'door', 'usa', 'hire'], ['posit', 'data', 'scienc', 'manag'], ['about', 'us'], ['who', 'look'], ['must', 'have'], ['what', 'offer'], ['We', 'look', 'excel', 'come', 'join', 'us'], ['100', 'remot', 'mid', 'level', 'big', 'data', 'analyst', 'develop', 'hadoop', 'nosql', 'sql', 'aerospac', 'dalla', 'TX', 'area', '1', 'year'], ['must', 'BE', 'A', 'U', 'S', 'citizen', 'qualifi', 'futur', 'secur', 'clearanc'], ['remot', 'covid', 'clear', 'must', 'abl', 'transit', 'onsit', 'road'], ['sql'], ['big', 'data'], ['visual'], ['larg'], ['complex', 'dataset'], ['sql', 'python'], ['java'], ['tableau'], ['big', 'data', 'hadoop'], ['nosql'], ['sql'], ['python'], ['java', 'C', 'C'], ['complex', 'high', 'volum', 'high', 'dimension', 'data'], ['enterpris', 'data', 'architectur'], ['must', 'BE', 'abl', '1', 'week', 'onsit', 'train', 'project', 'ramp'], ['predict', 'analyt'], ['descript', 'statist'], ['exploratori', 'data', 'analysi'], ['R', 'spss'], ['matlab'], ['F', '35', 'aerospac', 'avion', 'program'], ['senior', 'statistician', 'innov', 'biopharma', 'rayleigh', 'US'], ['about', 'role'], ['person', 'specif'], ['cultur'], ['what', 'get', 'return'], ['essenti', 'function'], ['minimum', 'educ'], ['minimum', 'experi'], ['requir', 'skill', 'abil', 'licensur'], ['other', 'job', 'descript', 'inform'], ['travel', 'posit'], ['address'], ['citi'], ['state'], ['zip', 'code'], ['domicil', 'locat'], ['eeo', 'statement'], ['eaton', 'corpor', 'current', 'seek', 'data', 'scienc', 'specialist', 'join', 'team', 'thi', 'posit', 'base', 'moon', 'township', 'PA', 'facil', 'reloc', 'assist', 'provid', 'need'], ['In', 'role'], ['qualif'], ['basic', 'qualif'], ['prefer', 'qualif'], ['posit', 'criteria'], ['job', 'engin'], ['region', 'north', 'america', 'US', 'puerto', 'rico'], ['organ', 'cto', 'corpor', 'technolog', 'offic'], ['job', 'level', 'individu', 'contributor'], ['schedul', 'full', 'time'], ['Is', 'remot', 'work', 'e', 'work', 'home', 'anoth', 'eaton', 'facil', 'allow', 'posit', 'No'], ['doe', 'posit', 'offer', 'reloc', 'reloc', 'within', 'hire', 'countri'], ['travel', 'ye', '10', 'time'], ['whi', 'amerisav'], ['amerisav', 'compani', 'wait', 'work'], ['requir'], ['core', 'account', 'skillset'], ['desir', 'skillset'], ['direct', 'w', '2', 'employ', 'No', '3rd', 'parti', 'posit', 'elig', 'visa', 'sponsorship', 'transfer'], ['lead', 'data', 'scientist'], ['what', 'look'], ['what'], ['what'], ['about', 'role'], ['key', 'respons'], ['requir', 'experi'], ['immedi', 'hire'], ['job', 'descript'], ['role', 'respons'], ['requir', 'skill'], ['desir', 'skill'], ['the', 'team'], ['locat'], ['job', 'type'], ['veterinari', 'technician'], ['respons'], ['requir'], ['commit', 'beyond', 'qualif'], ['your'], ['wellb'], ['matter'], ['potenti', 'big', 'your', 'passion'], ['support', 'help', 'thrive'], ['commit'], [], ['commun'], ['well'], ['So', 'wait'], ['the', 'team'], ['job', 'duti'], ['requir'], ['prefer'], ['skill'], ['qualif'], ['locat'], ['type'], ['experi'], ['function'], ['industri'], ['job', 'descript'], ['Ab', 'initio', 'data', 'engin'], ['data', 'integr', 'govern', 'qualiti'], ['technic', 'stack'], ['Ab', 'initio', '3', '5', 'x', '4', '0', 'x', 'softwar', 'suit'], ['Ab', 'initio', '3', '5', 'x', '4', '0', 'x', 'framework'], ['big', 'data'], ['databas'], ['other'], ['job', 'duti'], ['Ab', 'initio', 'graph'], ['conduct', 'plan'], ['web', 'servic', 'rest', 'graph'], ['metadata', 'hub', 'metamodel'], ['metadata', 'hub', 'oob', 'import', 'feed'], ['heterogen', 'data', 'sourc'], ['acquir', 'It', 'spec', 'To', 'graph', 'data', 'qualiti', 'assess'], ['CI', 'CD'], ['junit'], ['jenkin'], ['jira'], ['queri', 'It', 'data', 'sourc'], ['xml', 'json', 'yaml'], ['data', 'acquisit', 'transform', 'curat', 'requir'], ['control', 'center', 'job'], ['bre', 'ruleset'], ['perform', 'bottleneck'], ['perform', 'optim'], ['interoper', 'standard'], ['govern', 'polici'], ['regress'], ['agil', 'develop', 'process'], ['qualif'], ['5', 'year'], ['you', 'one', 'look'], ['about', 'iti'], ['locat'], ['type'], ['experi'], ['function'], ['industri'], ['job', 'descript'], ['senior', 'Ab', 'initio', 'solut', 'engin'], ['data', 'integr', 'govern', 'qualiti', 'domain'], ['creat', 'highli', 'optim', 'batch', 'real', 'time', 'applic'], ['acquir', 'curat', 'data', 'metadata', 'across', 'intern', 'extern', 'sourc'], ['technic', 'stack'], ['Ab', 'initio', '3', '5', 'x', '4', '0', 'x', 'softwar', 'suit'], ['Ab', 'initio', '3', '5', 'x', '4', '0', 'x', 'framework'], ['big', 'data'], ['CI', 'CD'], ['cloud'], ['databas'], ['other'], ['job', 'duti'], ['highli', 'optim', 'Ab', 'initio', 'graph'], ['conduct', 'plan'], ['gener', 'graph'], ['pdl', 'metaprogram'], ['gener', 'express', 'It', 'templat'], ['bre', 'ruleset'], ['web', 'servic', 'rest', 'model'], ['metadata', 'hub', 'metamodel'], ['view', 'custom', 'metadata', 'hub'], ['import', 'extractor', 'graph', 'metadata', 'hub'], ['heterogen', 'data', 'sourc'], ['acquir', 'It', 'spec', 'To', 'graph', 'data', 'qualiti', 'assess'], ['CI', 'CD'], ['junit'], ['jenkin'], ['jira'], ['queri', 'It', 'subgraph'], ['xml', 'json', 'yaml', 'document'], ['data', 'lake', 'warehous', 'data', 'model'], ['perform', 'bottleneck'], ['perform', 'optim'], ['interoper', 'standard'], ['govern', 'polici'], ['technic', 'design', 'document'], ['project'], ['liaison'], ['perform', 'data', 'access', 'requir'], ['agil', 'develop', 'process'], ['team', 'member'], ['qualif'], ['7', 'year'], ['5', 'year'], ['you', 'one', 'look'], ['about', 'iti'], ['100', 'remot', 'mid', 'level', 'big', 'data', 'analyst', 'develop', 'hadoop', 'nosql', 'sql', 'aerospac', 'dalla', 'TX', 'area', '1', 'year'], ['must', 'BE', 'A', 'U', 'S', 'citizen', 'qualifi', 'futur', 'secur', 'clearanc'], ['remot', 'covid', 'clear', 'must', 'abl', 'transit', 'onsit', 'road'], ['sql'], ['big', 'data'], ['visual'], ['larg'], ['complex', 'dataset'], ['sql', 'python'], ['java'], ['tableau'], ['big', 'data', 'hadoop'], ['nosql'], ['sql'], ['python'], ['java', 'C', 'C'], ['complex', 'high', 'volum', 'high', 'dimension', 'data'], ['enterpris', 'data', 'architectur'], ['must', 'BE', 'abl', '1', 'week', 'onsit', 'train', 'project', 'ramp'], ['predict', 'analyt'], ['descript', 'statist'], ['exploratori', 'data', 'analysi'], ['R', 'spss'], ['matlab'], ['F', '35', 'aerospac', 'avion', 'program'], ['overview'], ['qualif'], ['summari'], ['job', 'descript'], ['qualif'], ['experi', 'esi', 'process', 'product', 'tool'], ['must'], ['benefit'], ['who', 'We', 'are'], ['who', 'We', 'need'], ['what', 'you', 'will', 'Do'], ['what', 'you', 'are', 'like'], ['what', 'you', 'need'], ['nice', 'have'], ['what', 'We', 'offer'], ['leadership', 'experi'], ['technic', 'skill'], ['non', 'technic', 'requir']]\n"
     ]
    }
   ],
   "source": [
    "stemed_words = []\n",
    "for t in qualification.iloc[:,0]:\n",
    "    words = tokenizer.tokenize(t)\n",
    "    clean_text = pipeline(t)\n",
    "    stemed_words.append([porter.stem(w) for w in clean_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "pos = []\n",
    "keywords = ['experi','requir','qualif','skill', 'educ']\n",
    "for i, word in enumerate(stemed_words):\n",
    "    if len(set(word)&set(keywords)) != 0: \n",
    "        pos.append(i)\n",
    "        count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regex identifiers classified as yes: 72 and absolute values set 82\n"
     ]
    }
   ],
   "source": [
    "print(f'The regex identifiers classified as yes: {count} and absolute values set {(qualification.iloc[:,1].value_counts()[\"yes\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual checking to ensure correct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : QUALIFICATIONS\n",
      "2 : Education\n",
      "3 : Training and Experience\n",
      "4 : Health and Background Requirements\n",
      "9 : Requirements: \n",
      "16 : QUALIFICATIONS\n",
      "\n",
      "17 : SKILLS / COMPETENCIES\n",
      "\n",
      "18 : PHYSICAL REQUIREMENTS\n",
      "\n",
      "23 : REQUIRED EXPERIENCE, EDUCATION AND SKILLS\n",
      "24 : ADDITIONAL REQUIREMENTS\n",
      "25 : Responsibilities: \n",
      "30 : Physical Requirements \n",
      "41 : \n",
      "Persons Eligible to Apply\n",
      "44 : \n",
      "Requirements\n",
      "47 : Business Analyst Responsibilities: To include but not limited to the following\n",
      "48 : Business Analyst Minimum Requirements: \n",
      "49 : Business Analyst Abilities Required: \n",
      "52 : Qualifications: \n",
      "54 : \n",
      "Minimum Education\n",
      "55 : \n",
      "Minimum Experience\n",
      "56 : \n",
      "Required Skills, Abilities and / or Licensure\n",
      "66 : \n",
      "Minimum Education\n",
      "67 : \n",
      "Minimum Experience\n",
      "68 : \n",
      "Required Skills, Abilities and / or Licensure\n",
      "80 : Knowledge/Skills Required: \n",
      "81 : Qualifications (Education Requirements/Experience): \n",
      "87 : Qualifications\n",
      "88 : \n",
      "Qualifications\n",
      "89 : Minimum Job Requirements:\n",
      "90 : \n",
      "\n",
      "91 : Preferred Qualifications:\n",
      "103 : \n",
      "Essential Functions\n",
      "104 : \n",
      "Minimum Education\n",
      "105 : \n",
      "Minimum Experience\n",
      "116 : \n",
      "Responsibilities\n",
      "145 : Skills/Knowledge Considered a Plus:\n",
      "152 : DESCRIPTION\n",
      "159 : \n",
      "In this function you will:\n",
      "\n",
      "160 : \n",
      "Qualifications\n",
      "161 : \n",
      "Required (Basic) Qualifications:\n",
      "\n",
      "174 : DESCRIPTION\n",
      "185 : \n",
      "Relocation:\n",
      "192 : Staff Management and Financial Planning:\n",
      "193 : Qualifications\n",
      "194 : \n",
      "Minimum Qualifications:\n",
      "199 : Responsibilities: \n",
      "203 : Term: \n",
      "207 : \n",
      "Essential Functions\n",
      "208 : \n",
      "Minimum Education\n",
      "209 : \n",
      "Minimum Experience\n",
      "221 : About us: \n",
      "222 : Who we’re looking for: \n",
      "252 : About the Role\n",
      "256 : \n",
      "Essential Functions\n",
      "257 : \n",
      "Minimum Education\n",
      "258 : \n",
      "Minimum Experience\n",
      "269 : In this role you will:\n",
      "270 : Qualifications\n",
      "271 : \n",
      "Basic Qualifications:\n",
      "283 : \n",
      "AmeriSave is the company you’ve been waiting to work for!\n",
      "284 : Requirements\n",
      "285 : \n",
      "Core Accountabilities and Skillsets\n",
      "288 : Lead Data Scientist \n",
      "293 : \n",
      "Key Responsibilities:\n",
      "297 : Role and Responsibilities: \n",
      "298 : Required Skills: \n",
      "304 : \n",
      "Responsibilities:\n",
      "305 : \n",
      "Requirements:\n",
      "318 : Job Duties: \n",
      "319 : Requirements: \n",
      "320 : Preferred: \n",
      "321 : Skills\n",
      "359 : agile development process\n",
      "365 : Type: \n",
      "411 : team members\n",
      "414 : 5 years\n",
      "442 : Overview: \n",
      "445 : JOB DESCRIPTION: \n",
      "447 : Experience in ESI processing and production tools is a\n",
      "450 : Who We Are:\n",
      "454 : What You Need:\n",
      "458 : Technical Skills \n"
     ]
    }
   ],
   "source": [
    "for o in range(len(qualification.iloc[:,0])): \n",
    "    if qualification.iloc[:,1][o] == 'yes':\n",
    "        print(f'{o} : {qualification.iloc[:,0][o]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: QUALIFICATIONS\n",
      "3: Training and Experience\n",
      "4: Health and Background Requirements\n",
      "9: Requirements: \n",
      "12: Qualifications\n",
      "16: QUALIFICATIONS\n",
      "\n",
      "17: SKILLS / COMPETENCIES\n",
      "\n",
      "18: PHYSICAL REQUIREMENTS\n",
      "\n",
      "23: REQUIRED EXPERIENCE, EDUCATION AND SKILLS\n",
      "24: ADDITIONAL REQUIREMENTS\n",
      "30: Physical Requirements \n",
      "44: \n",
      "Requirements\n",
      "48: Business Analyst Minimum Requirements: \n",
      "49: Business Analyst Abilities Required: \n",
      "52: Qualifications: \n",
      "55: \n",
      "Minimum Experience\n",
      "56: \n",
      "Required Skills, Abilities and / or Licensure\n",
      "67: \n",
      "Minimum Experience\n",
      "68: \n",
      "Required Skills, Abilities and / or Licensure\n",
      "80: Knowledge/Skills Required: \n",
      "81: Qualifications (Education Requirements/Experience): \n",
      "87: Qualifications\n",
      "88: \n",
      "Qualifications\n",
      "89: Minimum Job Requirements:\n",
      "91: Preferred Qualifications:\n",
      "105: \n",
      "Minimum Experience\n",
      "106: \n",
      "Required Skills, Abilities and / or Licensure\n",
      "117: \n",
      "Requirements\n",
      "145: Skills/Knowledge Considered a Plus:\n",
      "146: \n",
      "Minimum Qualifications:\n",
      "153: PROFESSIONAL REQUIREMENTS\n",
      "160: \n",
      "Qualifications\n",
      "161: \n",
      "Required (Basic) Qualifications:\n",
      "\n",
      "162: Preferred qualifications:\n",
      "\n",
      "175: PROFESSIONAL REQUIREMENTS\n",
      "182: Deep understanding and strong experience in one or more of the following:\n",
      "183: \n",
      "Conceptual understanding and some experience in more than one of the following:\n",
      "186: \n",
      "Minimum Qualifications:\n",
      "190: Evaluate RE requirements and build out RE team to scale:\n",
      "193: Qualifications\n",
      "194: \n",
      "Minimum Qualifications:\n",
      "195: Preferred Qualifications:\n",
      "200: Requirements: \n",
      "204: Minimum Qualifications: \n",
      "209: \n",
      "Minimum Experience\n",
      "210: \n",
      "Required Skills, Abilities and / or Licensure\n",
      "258: \n",
      "Minimum Experience\n",
      "259: \n",
      "Required Skills, Abilities and / or Licensure\n",
      "270: Qualifications\n",
      "271: \n",
      "Basic Qualifications:\n",
      "272: Preferred Qualifications\n",
      "284: Requirements\n",
      "294: \n",
      "Requirements & Experience:\n",
      "298: Required Skills: \n",
      "299: Desired Skills: \n",
      "305: \n",
      "Requirements:\n",
      "306: Commitment beyond qualifications\n",
      "319: Requirements: \n",
      "321: Skills\n",
      "322: \n",
      "Qualifications\n",
      "325: Experience: \n",
      "351: data acquisition and transformation/curation requirements\n",
      "360: Qualifications: \n",
      "366: Experience: \n",
      "409: performance for data access requirements\n",
      "412: Qualifications: \n",
      "443: Qualifications: \n",
      "446: QUALIFICATIONS: \n",
      "447: Experience in ESI processing and production tools is a\n",
      "457: Leadership Experience \n",
      "458: Technical Skills \n",
      "459: Non-Technical Requirements\n"
     ]
    }
   ],
   "source": [
    "for p in pos: \n",
    "    print(f'{p}: {qualification.iloc[:,0][p]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model as a seperate entitiy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             JOB SUMMARY\n",
       "1                          QUALIFICATIONS\n",
       "2                               Education\n",
       "3                 Training and Experience\n",
       "4      Health and Background Requirements\n",
       "                      ...                \n",
       "455                        Nice to Haves:\n",
       "456                        What We Offer:\n",
       "457                Leadership Experience \n",
       "458                     Technical Skills \n",
       "459            Non-Technical Requirements\n",
       "Name: qualifications, Length: 460, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualification.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = ['Veterinary Technician']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>summary</th>\n",
       "      <th>qualifications</th>\n",
       "      <th>education</th>\n",
       "      <th>training</th>\n",
       "      <th>experience</th>\n",
       "      <th>health</th>\n",
       "      <th>background</th>\n",
       "      <th>requirements</th>\n",
       "      <th>covid</th>\n",
       "      <th>...</th>\n",
       "      <th>learning</th>\n",
       "      <th>management</th>\n",
       "      <th>system</th>\n",
       "      <th>specialist</th>\n",
       "      <th>general</th>\n",
       "      <th>key</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>work</th>\n",
       "      <th>environment</th>\n",
       "      <th>demands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job  summary  qualifications  education  training  experience  health  \\\n",
       "0    0        0               0          0         0           0       0   \n",
       "\n",
       "   background  requirements  covid  ...  learning  management  system  \\\n",
       "0           0             0      0  ...         0           0       0   \n",
       "\n",
       "   specialist  general  key  knowledge  work  environment  demands  \n",
       "0           0        0    0          0     0            0        0  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(test_string[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x13ae79370>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"jhjk\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match: United States\n",
      "Found match: U.S.\n",
      "Found match: US\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The a (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\.?) ?[Ss](tates|\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
